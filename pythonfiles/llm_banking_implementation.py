# -*- coding: utf-8 -*-
"""LLM_Banking_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tmnfz6sVhZQ8EqVmQ4l1dRkzDq-CUvxI

# **SECTION 01:** IMPORTING LIBRARIES ENV SET-UP
"""

!pip install --quiet openai
!pip install rouge-score -q
import pandas as pd
import numpy as np
import platform
import transformers
import sklearn
import torch
import torch.nn as nn
from transformers import AutoModel
import pickle
from transformers import AutoTokenizer
import torch.nn.functional as F
import ast
import os
from openai import OpenAI
from getpass import getpass
import json
from datetime import datetime
from rouge_score import rouge_scorer

print("=== Environment info ===")
print("Python        :", platform.python_version())
print("PyTorch       :", torch.__version__)
print("Transformers  :", transformers.__version__)
print("pandas        :", pd.__version__)
print("scikit-learn  :", sklearn.__version__)

# Expected Keys from Schema-constrained LLM Output and Validations

REQUIRED_KEYS = [
    "interaction_id",
    "timestamp",
    "mode",
    "summary",
    "actions",
    "clarifications",
    "risk_notes",
    "kb_policies_used",
]

ALLOWED_MODES = {"HIGH_CONFIDENCE", "REVIEW_REQUIRED"}
ALLOWED_URGENCY = {"low", "medium", "high", "critical"}


def validate_agent_json(raw_text: str):
    """
    Validate a single LLM output against the expected JSON schema.
    Returns (is_valid: bool, error_message: str | None).
    """
    try:
        data = json.loads(raw_text)
    except json.JSONDecodeError as e:
        return False, f"JSON decode error: {e}"

    if not isinstance(data, dict):
        return False, "Top-level JSON is not an object."

    missing = [k for k in REQUIRED_KEYS if k not in data]
    if missing:
        return False, f"Missing required keys: {missing}"

    if not isinstance(data["interaction_id"], str):
        return False, "interaction_id must be a string."

    if not isinstance(data["timestamp"], str):
        return False, "timestamp must be a string."

    if data["mode"] not in ALLOWED_MODES:
        return False, f"mode must be one of {sorted(ALLOWED_MODES)}."

    if not isinstance(data["summary"], str):
        return False, "summary must be a string."

    for key in ["actions", "clarifications", "risk_notes"]:
        val = data[key]
        if not isinstance(val, list):
            return False, f"{key} must be a list."
        if any(not isinstance(x, str) for x in val):
            return False, f"All items in {key} must be strings."

    kb_list = data["kb_policies_used"]
    if not isinstance(kb_list, list):
        return False, "kb_policies_used must be a list."
    for i, item in enumerate(kb_list):
        if not isinstance(item, dict):
            return False, f"kb_policies_used[{i}] must be an object."
        if "kb_id" not in item or "title" not in item:
            return False, f"kb_policies_used[{i}] must have 'kb_id' and 'title'."
        if not isinstance(item["kb_id"], str) or not isinstance(item["title"], str):
            return False, f"kb_policies_used[{i}].kb_id and title must be strings."

    return True, None

# ROUGE-L Scorer
rouge_scorer_obj = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)

def compute_rougeL(ref_text: str, pred_text: str):
    """
    Compute ROUGE-L F1 between reference and predicted text.
    Returns a float between 0 and 1.
    """
    scores = rouge_scorer_obj.score(ref_text, pred_text)
    return scores["rougeL"].fmeasure

"""# **SECTION 02:** RECREATING MODEL CLASS"""

# Recreate MultiTaskModel Class

class MultiTaskModel(nn.Module):
    def __init__(self, model_name, num_labels_dict, dropout=0.2):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        h = self.backbone.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.intent_head        = nn.Linear(h, num_labels_dict["intent"])
        self.issue_type_head    = nn.Linear(h, num_labels_dict["issue_type"])
        self.product_head       = nn.Linear(h, num_labels_dict["product"])
        self.urgency_head       = nn.Linear(h, num_labels_dict["urgency"])
        self.sentiment_head     = nn.Linear(h, num_labels_dict["sentiment"])
        self.routing_queue_head = nn.Linear(h, num_labels_dict["routing_queue"])

    def forward(self, input_ids, attention_mask):
        x = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]
        x = self.dropout(x)
        return {
            "intent":        self.intent_head(x),
            "issue_type":    self.issue_type_head(x),
            "product":       self.product_head(x),
            "urgency":       self.urgency_head(x),
            "sentiment":     self.sentiment_head(x),
            "routing_queue": self.routing_queue_head(x),
        }

"""# **SECTION 03:** LOADING MODEL, ENCODERS, LLM, KB AND EMBEDDINGS"""

# Load Model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

CKPT_PATH = "/content/drive/MyDrive/Colab Notebooks/TrainedModel/multitask_distilbert_clean.pt"

checkpoint = torch.load(CKPT_PATH, map_location=device)
model_name = checkpoint["model_name"]
num_labels_dict = checkpoint["num_labels_dict"]

model = MultiTaskModel(model_name, num_labels_dict).to(device)
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

backbone = model.backbone
backbone.eval()

tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Model reloaded successfully.")

# Load Encoders

ENCODER_PATH = "/content/drive/MyDrive/Colab Notebooks/TrainedModel/label_encoders.pkl"

with open(ENCODER_PATH, "rb") as f:
    encoders = pickle.load(f)

print("Encoders loaded. Keys:", list(encoders.keys()))

# Load KB

KB_PATH = "/content/drive/MyDrive/Colab Notebooks/DataSet/kb_policies_rag.csv"
kb_df = pd.read_csv(KB_PATH)

list_cols = ["product_tags", "issue_type_tags", "intent_tags"]
for col in list_cols:
    kb_df[col] = kb_df[col].apply(ast.literal_eval)

print("KB loaded. Shape:", kb_df.shape)

# Load KB Embeddings

KB_EMB_PATH = "/content/drive/MyDrive/Colab Notebooks/TrainedModel/kb_embeddings.pt"
kb_embeddings = torch.load(KB_EMB_PATH)
kb_embeddings_norm = F.normalize(kb_embeddings, p=2, dim=1)
print("KB embeddings built. Shape:", kb_embeddings.shape)

# Load API Key

api_key = getpass("Enter your OpenAI API key (hidden): ")
os.environ["OPENAI_API_KEY"] = api_key
client = OpenAI()

"""# **SECTION 04:** INTERACTIVE UI FOR PREIDCTIONS"""

model.to(device)
model.eval()
TASKS = ["intent","issue_type","product","urgency","sentiment","routing_queue"]
THRESHOLD = 0.40

d_channel = "mobile_banking"
d_segment = "retail_plus"

print("Press Enter to accept defaults.\n")

ch  = input(f"Channel (default={d_channel}): ").strip() or d_channel
seg = input(f"Segment (default={d_segment}): ").strip() or d_segment
ts  = input("Timestamp YYYY-MM-DD HH:MM:SS (default=NOW): ").strip()
sub = input("Subject (default=blank): ").strip()

txt = ""
while not txt.strip():
    txt = input("Ticket text (REQUIRED): ").strip()

if ts == "":
    t = pd.Timestamp.now()
else:
    t = pd.to_datetime(ts, errors="coerce")
    if pd.isna(t):
        t = pd.Timestamp.now()

dow = t.day_name()
h = t.hour
if 5 <= h < 12:
    tb = "morning"
elif 12 <= h < 17:
    tb = "afternoon"
elif 17 <= h < 22:
    tb = "evening"
else:
    tb = "night"
bh = "yes" if 8 <= h < 18 else "no"
we = "yes" if t.dayofweek >= 5 else "no"

parts = [
    f"[CHANNEL={ch}]",
    f"[SEGMENT={seg}]",
    f"[DOW={dow}]",
    f"[TIME_BUCKET={tb}]",
    f"[BUSINESS_HOURS={bh}]",
    f"[WEEKEND={we}]",
]
if sub:
    parts.append(f"[SUBJECT={sub}]")
parts.append(txt)
input_text = " ".join(parts)

tokenizer = AutoTokenizer.from_pretrained(model_name)
enc = tokenizer(
    input_text,
    truncation=True,
    padding="max_length",
    max_length=256,
    return_tensors="pt"
)
ids = enc["input_ids"].to(device)
mask = enc["attention_mask"].to(device)

with torch.no_grad():
    out = model(input_ids=ids, attention_mask=mask)

print("\n--- INPUT TO MODEL ---\n", input_text, "\n")

rows = []
for task in TASKS:
    logits = out[task][0]
    probs  = F.softmax(logits, dim=0)
    pred_idx = int(torch.argmax(probs).item())
    conf = float(probs[pred_idx].item())

    if "encoders" in globals():
        label = encoders[task].inverse_transform([pred_idx])[0]
    else:
        label = "<encoder not available>"

    status = "Review Required" if conf < THRESHOLD else "OK"

    rows.append({
        "Task": task,
        "Predicted ID": pred_idx,
        "Predicted Label": label,
        "Confidence (%)": round(conf * 100, 1),
        "Tag": status
    })

df_results = pd.DataFrame(rows)
print(df_results.to_string(index=False))

"""# **SECTION 05:** RETIREVE KB POLICIES"""

# Query Embedding
@torch.no_grad()
def embed_query(text, max_length=256):
    enc = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length
    ).to(device)
    out = backbone(**enc).last_hidden_state
    emb = out[:, 0, :]
    emb = F.normalize(emb, p=2, dim=1)
    return emb

# Retrieval
@torch.no_grad()
def retrieve_kb(query_text, top_k=3):
    query_emb = embed_query(query_text)
    sims = torch.matmul(kb_embeddings_norm.to(device), query_emb[0])

    top_vals, top_idx = torch.topk(sims, k=top_k)
    kb_hits = kb_df.iloc[top_idx.cpu().numpy()].copy()
    kb_hits["similarity"] = top_vals.cpu().numpy()
    return kb_hits

def rerank_kb_hits(kb_hits, pred_labels):
    def score_row(row):
        score = row["similarity"]

        if pred_labels["product"] in row["product_tags"]:
            score += 0.10
        if pred_labels["issue_type"] in row["issue_type_tags"]:
            score += 0.10
        if pred_labels["intent"] in row["intent_tags"]:
            score += 0.05
        return score

    kb_hits = kb_hits.copy()
    kb_hits["final_score"] = kb_hits.apply(score_row, axis=1)
    return kb_hits.sort_values("final_score", ascending=False)

# Retrieve Labels and Tags from ML Model
tasks = ["intent", "issue_type", "product", "urgency", "sentiment", "routing_queue"]

model.eval()
with torch.no_grad():
    enc = tokenizer(
        input_text,
        padding=True,
        truncation=True,
        max_length=256,
        return_tensors="pt"
    ).to(device)
    input_ids = enc["input_ids"]
    attention_mask = enc["attention_mask"]
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)

pred_labels = {}
pred_tags = {}
confidences = {}

for t in tasks:
    logits = outputs[t]
    probs = torch.softmax(logits, dim=-1)[0]
    conf, idx = torch.max(probs, dim=0)
    conf = float(conf.item())
    label_idx = int(idx.item())

    label = encoders[t].inverse_transform([label_idx])[0]
    pred_labels[t] = label
    confidences[t] = conf
    pred_tags[t] = "OK" if conf >= 0.40 else "Review Required"

# Build query_text from Labels + Original ticket
query_text = f"""
Intent: {pred_labels['intent']}
Issue type: {pred_labels['issue_type']}
Product: {pred_labels['product']}
Urgency: {pred_labels['urgency']}
Sentiment: {pred_labels['sentiment']}
Routing queue: {pred_labels['routing_queue']}

Ticket:
{input_text}
""".strip()

# Retrieve KB Snippets
kb_hits_raw = retrieve_kb(query_text, top_k=5)
kb_hits = rerank_kb_hits(kb_hits_raw, pred_labels).head(3)

print("=== KB retrieval query text ===")
print(query_text)

print("\n=== Top KB matches for this ticket (agent view) ===")
for _, row in kb_hits.iterrows():
    print(f"\n[KB_ID: {row['kb_id']}]")
    print(f"Similarity   : {row['similarity']:.3f}")
    print(f"Final score  : {row['final_score']:.3f}")
    print(f"Title        : {row['title']}")
    print(f"Routing queue: {row['routing_queue']}")
    print("Body:")
    print(row["body_text"])

"""# **SECTION 06:** BUILDING LLM PROMPT"""

# Building LLM Guided Prompt

def build_agent_guidance_prompt(
    input_text,
    pred_labels,
    pred_tags,
    confidences,
    kb_hits,
    max_kb=2
):

    # Create Unique ID + Timestamp + Mode
    now = datetime.now()
    interaction_id = now.strftime("%d%m%y%H%M%S%f")[:17]
    timestamp_str = now.strftime("%d/%m/%y %H:%M:%S")
    all_ok = all(pred_tags[t] == "OK" for t in pred_tags)
    mode = "HIGH_CONFIDENCE" if all_ok else "REVIEW_REQUIRED"

    lines = []

    lines.append("=== LOG DATA ===")
    lines.append(f"INTERACTION_ID: {interaction_id}")
    lines.append(f"CURRENT_DATETIME: {timestamp_str}")
    lines.append(f"MODE: {mode}\n")

    # LLM Role and Guidance
    lines.append("=== LLM CONTEXT ===")
    lines.append("You are an internal AI copilot for a banking support agent. Never talk to the customer.")
    lines.append("Use ONLY the MODEL PREDICTIONS and RETRIEVED KB SNIPPETS for all actions, clarifications and risk_notes.")
    lines.append("Do NOT invent new policies, products, fees, legal terms, timeframes or guarantees.")
    lines.append("If something is not clearly covered in the KB, output clarifying questions instead of guessing.")
    lines.append("Reply with a single valid JSON object only (no markdown, no extra text).")
    lines.append("Use double-quoted keys/strings, and in kb_policies_used reference KB items by their IDs and titles.\n")

    # Ticket Data
    lines.append("=== TICKET DATA ===")
    lines.append(f"{input_text}\n")

    # Model Predictions
    lines.append("=== MODEL PREDICTIONS ===")
    for t in ["intent","issue_type","product","urgency","sentiment","routing_queue"]:
        lines.append(
            f"- {t}: {pred_labels[t]} "
            f"(conf={confidences[t]:.2f}, tag={pred_tags[t]})"
        )

    # KB Snippets
    lines.append("\n=== RETRIEVED KB SNIPPETS ===")
    kb_refs = []
    for i, (_, row) in enumerate(kb_hits.head(max_kb).iterrows(), start=1):
        body = row["body_text"]
        if len(body) > 600:
            body = body[:600] + " ..."
        kb_label = f"KB{i}"
        kb_id    = row["kb_id"]
        kb_title = row["title"]

        kb_refs.append(f"{kb_label} – {kb_title}")

        lines.append(f"[{kb_label}] ID={kb_id}  Title={kb_title}")
        lines.append(body)
        lines.append("")

    if kb_refs:
        lines.append("KB_LABELS_SUMMARY:")
        for ref in kb_refs:
            lines.append(f"- {ref}")
        lines.append("")

    # Schema Instructions
    lines.append("=== OUTPUT SCHEMA ===")
    lines.append(
        "Return ONLY a JSON object of this exact form (no extra text):\n"
        "{\n"
        '  "interaction_id": "string – copy INTERACTION_ID exactly",\n'
        '  "timestamp": "string – copy CURRENT_DATETIME exactly",\n'
        '  "mode": "HIGH_CONFIDENCE or REVIEW_REQUIRED",\n'
        '  "summary": "one-sentence internal summary of the case",\n'
       #'  "predicted_intent": "copy the intent label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_intent_confidence": 0.00,\n'
       #'  "predicted_issue_type": "copy the issue_type label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_issue_type_confidence": 0.00,\n'
       #'  "predicted_product": "copy the product label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_product_confidence": 0.00,\n'
       #'  "predicted_urgency": "copy the urgency label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_urgency_confidence": 0.00,\n'
       #'  "predicted_sentiment": "copy the sentiment label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_sentiment_confidence": 0.00,\n'
       #'  "predicted_routing_queue": "copy the routing_queue label exactly as shown in MODEL PREDICTIONS",\n'
       #'  "predicted_routing_queue_confidence": 0.00,\n'
        '  "actions": ["2 to 4 short action strings for the agent"],\n'
        '  "clarifications": ["0 to 4 short questions to ask the customer"],\n'
        '  "risk_notes": ["0 to 3 very short risk/compliance notes"],\n'
        '  "kb_policies_used": [\n'
        '    {"kb_id": "string", "title": "string"},\n'
        '    {"kb_id": "string", "title": "string"}\n'
        "  ]\n"
        "}"
    )

    return "\n".join(lines)

# Build Prompt
agent_prompt = build_agent_guidance_prompt(
    input_text=input_text,
    pred_labels=pred_labels,
    pred_tags=pred_tags,
    confidences=confidences,
    kb_hits=kb_hits,
    max_kb=2,
)

print("================================== LLM GUIDED PROMPT REQUEST ==================================")
print(agent_prompt)

"""# **SECTION 07:** GENERATING LLM RESPONSE"""

# Call LLM API
response = client.chat.completions.create(
    model="gpt-5-mini-2025-08-07",
    messages=[
        {"role": "user", "content": agent_prompt},
    ],
    temperature=1,
)

agent_guidance_raw = response.choices[0].message.content
#print("\n=== RAW LLM OUTPUT ===")
#print(agent_guidance_raw)

# Parse JSON
try:
    agent_guidance = json.loads(agent_guidance_raw)
except json.JSONDecodeError as e:
    print("\n[ERROR] JSON parsing failed. Raw output was:\n")
    print(agent_guidance_raw)
    raise e

print("\n================================== LLM SCHEMA-CONSTRAINED RESPONSE ==================================")
print(json.dumps(agent_guidance, indent=2))

"""# **SECTION 08:** ROUGE-L EVALUATION AND VALIDATION"""

# Rouge-L Evaluation
print("=== LLM-Generated Actions ===")
for i, a in enumerate(agent_guidance.get("actions", []), start=1):
    print(f"{i}. {a}")

reference_actions_text = input(
    "\nReference Actions:\n"
)

generated_actions_text = " ".join(agent_guidance.get("actions", []))

rougeL_f1 = compute_rougeL(reference_actions_text, generated_actions_text)
print(f"\nROUGE-L F1: {rougeL_f1:.3f}")

# Validate JSON Structure
is_valid, error = validate_agent_json(agent_guidance_raw)

if is_valid:
    print("\nJSON schema validation: PASS")
else:
    print("\nJSON schema validation: FAIL")
    print("Reason:", error)

num_tickets = 1
run_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

print("\n=== LLM RUN CONFIGURATION ===")
print(f"Date/time              : {run_time}")
print(f"Model name             : {model}")
print(f"Number of tickets eval : {num_tickets}")

# ROUGE-L Distribution (Histogram)
df = pd.read_csv("/content/drive/MyDrive/University Documents/LLM_eval_results.csv")

rouge_vals = df["rougeL_f1"].dropna().values
mean_rouge = rouge_vals.mean()
std_rouge  = rouge_vals.std()

fig, ax = plt.subplots(figsize=(4.5, 3), dpi=1000)

bins = np.linspace(rouge_vals.min() - 0.005, rouge_vals.max() + 0.005, 10)

ax.hist(
    rouge_vals,
    bins=bins,
    color="#00008b",
    edgecolor="white",
    linewidth=1
)

ax.axvline(
    mean_rouge,
    color="#1f4e79",
    linestyle="--",
    linewidth=1.0,
    label=f"Mean = {mean_rouge:.3f}"
)

ax.axvline(
    mean_rouge - std_rouge,
    color="#9bbadf",
    linestyle="--",
    linewidth=0.8,
    label=f"Mean - 1σ = {mean_rouge - std_rouge:.3f}"
)

ax.axvline(
    mean_rouge + std_rouge,
    color="#9bbadf",
    linestyle="--",
    linewidth=0.8,
    label=f"Mean + 1σ = {mean_rouge + std_rouge:.3f}"
)

ax.set_xlabel("ROUGE-L F1 score", fontsize=7)
ax.set_ylabel("Number of tickets", fontsize=7)
ax.set_title("ROUGE-L Distribution (Histogram)", fontsize=8)

ax.tick_params(axis="both", labelsize=6)
ax.grid(axis="y", linestyle=":", linewidth=0.4, alpha=0.7)

ax.legend(fontsize=6, loc="upper left", frameon=True)

plt.tight_layout()
plt.show()