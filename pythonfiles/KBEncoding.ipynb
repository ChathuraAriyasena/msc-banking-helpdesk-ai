{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cP0tRAZ3BW_"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the MultiTaskModel class\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels_dict, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        h = self.backbone.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.intent_head        = nn.Linear(h, num_labels_dict[\"intent\"])\n",
        "        self.issue_type_head    = nn.Linear(h, num_labels_dict[\"issue_type\"])\n",
        "        self.product_head       = nn.Linear(h, num_labels_dict[\"product\"])\n",
        "        self.urgency_head       = nn.Linear(h, num_labels_dict[\"urgency\"])\n",
        "        self.sentiment_head     = nn.Linear(h, num_labels_dict[\"sentiment\"])\n",
        "        self.routing_queue_head = nn.Linear(h, num_labels_dict[\"routing_queue\"])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        return {\n",
        "            \"intent\":        self.intent_head(x),\n",
        "            \"issue_type\":    self.issue_type_head(x),\n",
        "            \"product\":       self.product_head(x),\n",
        "            \"urgency\":       self.urgency_head(x),\n",
        "            \"sentiment\":     self.sentiment_head(x),\n",
        "            \"routing_queue\": self.routing_queue_head(x),\n",
        "        }"
      ],
      "metadata": {
        "id": "8V-8YBuApzT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === 1. Load trained checkpoint (SAME as inference) ===\n",
        "CKPT_PATH = \"/content/drive/MyDrive/Colab Notebooks/TrainedModel/multitask_distilbert_clean.pt\"\n",
        "\n",
        "checkpoint = torch.load(CKPT_PATH, map_location=device)\n",
        "model_name = checkpoint[\"model_name\"]\n",
        "num_labels_dict = checkpoint[\"num_labels_dict\"]\n",
        "\n",
        "model = MultiTaskModel(model_name, num_labels_dict).to(device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "backbone = model.backbone\n",
        "backbone.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Model reloaded successfully.\")\n",
        "\n",
        "# === 2. Load KB CSV ===\n",
        "KB_PATH = \"/content/drive/MyDrive/Colab Notebooks/DataSet/kb_policies_rag.csv\"\n",
        "kb_df = pd.read_csv(KB_PATH)\n",
        "\n",
        "list_cols = [\"product_tags\", \"issue_type_tags\", \"intent_tags\"]\n",
        "for col in list_cols:\n",
        "    kb_df[col] = kb_df[col].apply(ast.literal_eval)\n",
        "\n",
        "print(\"KB loaded. Shape:\", kb_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBnonoEAD4fZ",
        "outputId": "e056bae9-6468-42d4-edba-b57c726b4960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model reloaded successfully.\n",
            "KB loaded. Shape: (22, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 3. Embedding function using model.backbone ===\n",
        "@torch.no_grad()\n",
        "def embed_texts(texts, max_length=256, batch_size=16):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    all_embs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "        out = backbone(**enc).last_hidden_state   # [B, T, H]\n",
        "        emb = out[:, 0, :]                        # CLS pooling (be consistent!)\n",
        "        all_embs.append(emb.cpu())\n",
        "    embs = torch.cat(all_embs, dim=0)\n",
        "    return embs      # save unnormalised; weâ€™ll normalise on load\n",
        "\n",
        "# === 4. Build and save KB embeddings ===\n",
        "kb_texts = kb_df[\"body_text\"].tolist()\n",
        "kb_embeddings = embed_texts(kb_texts)\n",
        "\n",
        "KB_EMB_PATH = \"/content/drive/MyDrive/Colab Notebooks/TrainedModel/kb_embeddings.pt\"\n",
        "torch.save(kb_embeddings, KB_EMB_PATH)\n",
        "\n",
        "print(\"KB embeddings built and saved to:\", KB_EMB_PATH)\n",
        "print(\"KB embeddings shape:\", kb_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Ms74jiEBHK",
        "outputId": "281a7cf0-64ad-4965-83f0-40fbb56916a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KB embeddings built and saved to: /content/drive/MyDrive/Colab Notebooks/TrainedModel/kb_embeddings.pt\n",
            "KB embeddings shape: torch.Size([22, 768])\n"
          ]
        }
      ]
    }
  ]
}