# -*- coding: utf-8 -*-
"""ML_Banking_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8aztindT66AJmD6oevdjbIom2d-_C0Z

# **SECTION 01:** IMPORTING LIBRARIES AND DATASET
"""

!pip install -q pandas
!pip install -q matplotlib
!pip install -q scikit-learn
!pip install -q torch transformers
!pip install -q tqdm

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.colors import Normalize
from numpy.polynomial import Polynomial

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

import torch
from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from transformers import AutoModel
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup
from tqdm.auto import tqdm
import torch.nn.functional as F

import pickle
import textwrap
import random
import platform
import transformers
import sklearn

# Reproducibility Controls

SEED = 42

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print(f"Reproducibility seed set to: {SEED}")
print("=== Environment info ===")
print("Python        :", platform.python_version())
print("PyTorch       :", torch.__version__)
print("Transformers  :", transformers.__version__)
print("pandas        :", pd.__version__)
print("scikit-learn  :", sklearn.__version__)

# Loading Dataset and Sanity Check

drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/Colab Notebooks/DataSet/banking_support_tickets.csv"

df = pd.read_csv(file_path)

print("Shape:", df.shape)
print("\nColumns:", df.columns.tolist())

print("\nFirst 5 rows:")
display(df.head())

print("\nChannel Distribution:")
print(df["channel"].value_counts())

print("\nQuick info:")
df.info()

"""# **SECTION 02:** DATA PRE-PROCESSING"""

# Define Basic Global Cleaning Rules

# Channels Considered Valid
VALID_CHANNELS = {
    "mobile_banking",
    "internet_banking",
    "chatbot",
    "call_center",
    "branch",
    "email",
    "whatsapp",
}

# Required Columns for Modeling
REQUIRED_COLS = [
    "ticket_text",
    "channel",
    "customer_segment",
    "intent",
    "issue_type",
    "product",
    "urgency",
    "sentiment",
    "routing_queue",
]

# drop rows with missing required fields
df_cleaned = df.dropna(subset=REQUIRED_COLS).copy()

# drop rows with empty ticket_text
df_cleaned = df_cleaned[df_cleaned["ticket_text"].str.strip().str.len() > 5]

# drop rows with invalid channels
df_cleaned = df_cleaned[df_cleaned["channel"].isin(VALID_CHANNELS)]

# Parse timestamp into datetime column
df_cleaned["timestamp_parsed"] = pd.to_datetime(df_cleaned["timestamp"], errors="coerce")
invalid_count = df_cleaned["timestamp_parsed"].isna().sum()
df_cleaned = df_cleaned[df_cleaned["timestamp_parsed"].notna()].copy()

df_cleaned = df_cleaned[df_cleaned["data_source"] != "corrupted"].copy()

print("Rows after global cleaning:", len(df_cleaned))
print("\nRows by data_source AFTER cleaning:")
print(df_cleaned["data_source"].value_counts())

# Create Time Features

df_cleaned["day_of_week"] = df_cleaned["timestamp_parsed"].dt.day_name()

def time_of_day_bucket(hour):
    if 5 <= hour < 12:
        return "morning"
    elif 12 <= hour < 17:
        return "afternoon"
    elif 17 <= hour < 22:
        return "evening"
    else:
        return "night"

df_cleaned["time_of_day_bucket"] = df_cleaned["timestamp_parsed"].dt.hour.apply(time_of_day_bucket)

def is_business_hours(hour):
    return "yes" if 8 <= hour < 18 else "no"

df_cleaned["is_business_hours"] = df_cleaned["timestamp_parsed"].dt.hour.apply(is_business_hours)

df_cleaned["is_weekend"] = df_cleaned["timestamp_parsed"].dt.dayofweek.apply(
    lambda d: "yes" if d >= 5 else "no"
)

print(df_cleaned[["timestamp_parsed", "day_of_week", "time_of_day_bucket",
                  "is_business_hours", "is_weekend"]].head())

# Verify Raw Counts
df_clean_final = df_cleaned[df_cleaned["data_source"] == "clean"].copy()
df_noisy_final = df_cleaned[df_cleaned["data_source"] == "noisy"].copy()

print("Rows in each working set:")
print("  CLEAN final :", len(df_clean_final))
print("  NOISY final :", len(df_noisy_final))

print("=== Dataset Sizes after Preprocessing ===")

n_total = len(df)
n_clean = len(df_clean_final)
n_noisy = len(df_noisy_final)
n_used  = n_clean + n_noisy
n_dropped = n_total - n_used

print(f"Total Tickets in Dataset : {n_total}")
print(f"Final CLEAN Tickets Used for ML   : {n_clean}")
print(f"Final NOISY Tickets (robustness)  : {n_noisy}")
print(f"DROPPED / CORRUPTED Tickets       : {n_dropped}")

"""# **SECTION 03:** DATA ANALYSIS AND VISUALIZATION"""

# Total Counts Before vs After Cleaning
before_total = len(df)
after_total  = len(df_cleaned)

x = np.arange(2)
labels_bar = ["Before\ncleaning", "After\ncleaning"]
bar_values = [before_total, after_total]

# Blue shades
dark_blue   = "#00008b"
mid_blue    = "#1e90ff"
light_blue1 = "#87cefa"
light_blue2 = "#add8e6"

# Channel Distribution
channel_counts = df_cleaned["channel"].value_counts()
channel_labels = channel_counts.index.tolist()
channel_values = channel_counts.values
channel_total  = channel_values.sum()
channel_pcts   = channel_values / channel_total * 100
channel_labels_fmt = [
    f"{lbl} ({pct:.1f}%)" for lbl, pct in zip(channel_labels, channel_pcts)
]

channel_colors = [dark_blue, mid_blue, light_blue1, light_blue2] * (
    len(channel_labels_fmt) // 4 + 1
)
channel_colors = channel_colors[:len(channel_labels_fmt)]

# Customer Segment Distribution
seg_counts = df_cleaned["customer_segment"].value_counts()
seg_labels = seg_counts.index.tolist()
seg_values = seg_counts.values
seg_total  = seg_values.sum()
seg_pcts   = seg_values / seg_total * 100
seg_labels_fmt = [
    f"{lbl} ({pct:.1f}%)" for lbl, pct in zip(seg_labels, seg_pcts)
]

seg_colors = [dark_blue, mid_blue, light_blue1, light_blue2] * (
    len(seg_labels_fmt) // 4 + 1
)
seg_colors = seg_colors[:len(seg_labels_fmt)]

fig = plt.figure(figsize=(10, 3.5), dpi=250)
gs = fig.add_gridspec(1, 3, width_ratios=[0.6, 1.2, 1.2])

ax_bar  = fig.add_subplot(gs[0, 0])
ax_pie1 = fig.add_subplot(gs[0, 1])
ax_pie2 = fig.add_subplot(gs[0, 2])

bar_colors = [dark_blue, light_blue1]

ax_bar.bar(x, bar_values, color=bar_colors, width=0.5)

ax_bar.set_xticks(x)
ax_bar.set_xticklabels(labels_bar, rotation=45, ha="right", fontsize=7)
ax_bar.set_ylabel("Ticket count", fontsize=7)
ax_bar.set_title("Total tickets\nBefore vs After cleaning", fontsize=7)
ax_bar.tick_params(axis="y", labelsize=7)

for i, v in enumerate(bar_values):
    ax_bar.text(
        i,
        v,
        f"{v}",
        ha="center",
        va="bottom",
        fontsize=6
    )

ax_bar.margins(x=0.2)

wedges1, texts1 = ax_pie1.pie(
    channel_values,
    labels=channel_labels_fmt,
    startangle=90,
    colors=channel_colors,
    labeldistance=1.15,  # push labels outside
    textprops={"fontsize": 6},
    wedgeprops={"edgecolor": "white", "linewidth": 0.5},
)

ax_pie1.axis("equal")
ax_pie1.set_title("Channel distribution (cleaned)", fontsize=7)

wedges2, texts2 = ax_pie2.pie(
    seg_values,
    labels=seg_labels_fmt,
    startangle=90,
    colors=seg_colors,
    labeldistance=1.15,
    textprops={"fontsize": 6},
    wedgeprops={"edgecolor": "white", "linewidth": 0.5},
)

ax_pie2.axis("equal")
ax_pie2.set_title("Customer segment distribution (cleaned)", fontsize=7)

plt.tight_layout()
plt.show()

TOP_N_ISSUE_TYPES = 20  # top 20 issue types only

dark_blue   = "#00008b"
mid_blue    = "#1e90ff"
light_blue1 = "#87cefa"
light_blue2 = "#add8e6"

# INTENT
intent_counts = df_cleaned["intent"].value_counts()
intent_total  = intent_counts.sum()
intent_pct    = (intent_counts / intent_total) * 100

intent_labels = intent_pct.index.tolist()
intent_values = intent_pct.values

# PRODUCT
product_counts = df_cleaned["product"].value_counts()
product_total  = product_counts.sum()
product_pct    = (product_counts / product_total) * 100

product_labels = product_pct.index.tolist()
product_values = product_pct.values

# ISSUE TYPE
issue_counts_full = df_cleaned["issue_type"].value_counts()
issue_counts = issue_counts_full.head(TOP_N_ISSUE_TYPES)

issue_total  = issue_counts.sum()
issue_pct    = (issue_counts / issue_total) * 100

issue_labels = issue_pct.index.tolist()
issue_values = issue_pct.values

# ROUTING QUEUE
rq_counts = df_cleaned["routing_queue"].value_counts()
rq_total  = rq_counts.sum()
rq_pct    = (rq_counts / rq_total) * 100

rq_labels = rq_pct.index.tolist()
rq_values = rq_pct.values

# URGENCY
urg_counts = df_cleaned["urgency"].value_counts()
urg_order = ["low", "medium", "high", "critical"]
urg_values = np.array([urg_counts.get(u, 0) for u in urg_order])
urg_total  = urg_values.sum()
urg_pct    = (urg_values / urg_total) * 100 if urg_total > 0 else np.zeros_like(urg_values)
urg_labels = [f"{u} ({p:.1f}%)" for u, p in zip(urg_order, urg_pct)]

urg_colors = [light_blue2, light_blue1, mid_blue, dark_blue]

# SENTIMENT
sent_counts = df_cleaned["sentiment"].value_counts()
sent_labels_raw = sent_counts.index.tolist()
sent_values = sent_counts.values
sent_total  = sent_values.sum()
sent_pct    = sent_values / sent_total * 100 if sent_total > 0 else np.zeros_like(sent_values)
sent_labels = [f"{lbl} ({pct:.1f}%)" for lbl, pct in zip(sent_labels_raw, sent_pct)]

sent_colors = [dark_blue, mid_blue, light_blue1][:len(sent_labels)]

fig = plt.figure(figsize=(10, 9), dpi=250)
gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1])

ax_intent  = fig.add_subplot(gs[0, 0])
ax_product = fig.add_subplot(gs[0, 1])

ax_issue   = fig.add_subplot(gs[1, 0])
ax_rq      = fig.add_subplot(gs[1, 1])

ax_urg     = fig.add_subplot(gs[2, 0])
ax_sent    = fig.add_subplot(gs[2, 1])

x_intent = np.arange(len(intent_labels))
ax_intent.bar(x_intent, intent_values, color=mid_blue)
ax_intent.set_xticks(x_intent)
ax_intent.set_xticklabels(intent_labels, rotation=45, ha="right", fontsize=6)
ax_intent.set_ylabel("Percentage of tickets (%)", fontsize=7)
ax_intent.set_title("Intent distribution", fontsize=8)
ax_intent.tick_params(axis="y", labelsize=6)

for i, v in enumerate(intent_values):
    ax_intent.text(
        i,
        v + 0.5,
        f"{v:.1f}%",
        ha="center",
        va="bottom",
        fontsize=6,
    )

x_prod = np.arange(len(product_labels))
ax_product.bar(x_prod, product_values, color=dark_blue)
ax_product.set_xticks(x_prod)
ax_product.set_xticklabels(product_labels, rotation=45, ha="right", fontsize=6)
ax_product.set_ylabel("Percentage of tickets (%)", fontsize=7)
ax_product.set_title("Product distribution", fontsize=8)
ax_product.tick_params(axis="y", labelsize=6)

for i, v in enumerate(product_values):
    ax_product.text(
        i,
        v + 0.5,
        f"{v:.1f}%",
        ha="center",
        va="bottom",
        fontsize=6,
    )

y_issue = np.arange(len(issue_labels))
ax_issue.barh(y_issue, issue_values, color=light_blue1)
ax_issue.set_yticks(y_issue)
ax_issue.set_yticklabels(issue_labels, fontsize=6)
ax_issue.invert_yaxis()
ax_issue.set_xlabel("Percentage of tickets (%)", fontsize=7)
ax_issue.set_title(f"Issue type distribution (Top {TOP_N_ISSUE_TYPES})", fontsize=8)
ax_issue.tick_params(axis="x", labelsize=6)

for i, v in enumerate(issue_values):
    ax_issue.text(
        v + 0.5,
        i,
        f"{v:.1f}%",
        va="center",
        fontsize=6,
    )

y_rq = np.arange(len(rq_labels))
ax_rq.barh(y_rq, rq_values, color=mid_blue)
ax_rq.set_yticks(y_rq)
ax_rq.set_yticklabels(rq_labels, fontsize=6)
ax_rq.invert_yaxis()
ax_rq.set_xlabel("Percentage of tickets (%)", fontsize=7)
ax_rq.set_title("Routing queue distribution", fontsize=8)
ax_rq.tick_params(axis="x", labelsize=6)

for i, v in enumerate(rq_values):
    ax_rq.text(
        v + 0.5,
        i,
        f"{v:.1f}%",
        va="center",
        fontsize=6,
    )

if urg_total > 0:
    wedges_u, texts_u = ax_urg.pie(
        urg_values,
        labels=urg_labels,
        startangle=90,
        colors=urg_colors,
        labeldistance=1.15,
        textprops={"fontsize": 6},
        wedgeprops={"edgecolor": "white", "linewidth": 0.5},
    )
ax_urg.axis("equal")
ax_urg.set_title("Urgency distribution", fontsize=8)

if sent_total > 0:
    wedges_s, texts_s = ax_sent.pie(
        sent_values,
        labels=sent_labels,
        startangle=90,
        colors=sent_colors,
        labeldistance=1.15,
        textprops={"fontsize": 6},
        wedgeprops={"edgecolor": "white", "linewidth": 0.5},
    )

    centre_circle = plt.Circle((0, 0), 0.60, fc="white")
    ax_sent.add_artist(centre_circle)

ax_sent.axis("equal")
ax_sent.set_title("Sentiment distribution", fontsize=8)

plt.tight_layout()
plt.show()

product_palette = [
    "#081d58",
    "#08306b",
    "#08519c",
    "#2171b5",
    "#4292c6",
    "#6baed6",
    "#9ecae1",
    "#c6dbef",
    "#2b8cbe",
    "#7bccc4",
]

def get_product_colors(n):
    return [product_palette[i % len(product_palette)] for i in range(n)]

blue_shades = [
    "#081d58",
    "#08306b",
    "#08519c",
    "#2171b5",
    "#4292c6",
    "#6baed6",
    "#9ecae1",
    "#c6dbef",
    "#deebf7",
]

def get_blue_shades(n):
    return [blue_shades[i % len(blue_shades)] for i in range(n)]

cmap_blues = cm.get_cmap("Blues")

# Tickets by day_of_week
dow_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
dow_counts = df_cleaned["day_of_week"].value_counts().reindex(dow_order, fill_value=0)

# Channel × time_of_day_bucket
ct_channel_time = pd.crosstab(df_cleaned["channel"], df_cleaned["time_of_day_bucket"])

channels = ct_channel_time.index.tolist()
time_buckets = ct_channel_time.columns.tolist()
x_time = np.arange(len(time_buckets))

# Customer segment × product
ct_seg_prod = pd.crosstab(df_cleaned["customer_segment"], df_cleaned["product"])
ct_seg_prod_pct = ct_seg_prod.div(ct_seg_prod.sum(axis=1), axis=0) * 100

segments = ct_seg_prod_pct.index.tolist()
products = ct_seg_prod_pct.columns.tolist()

# Sentiment × urgency (heatmap)
urg_order = ["low", "medium", "high", "critical"]
sent_order = ["very_negative", "negative", "neutral"]

ct_sent_urg = pd.crosstab(df_cleaned["sentiment"], df_cleaned["urgency"])
ct_sent_urg = ct_sent_urg.reindex(index=sent_order, columns=urg_order, fill_value=0)

sent_labels = ct_sent_urg.index.tolist()
urg_labels = ct_sent_urg.columns.tolist()
heat_sent_urg = ct_sent_urg.values

fig, axes = plt.subplots(2, 2, figsize=(10, 7), dpi=250)
ax_dow, ax_ct, ax_seg_prod, ax_sent_urg = axes[0, 0], axes[0, 1], axes[1, 0], axes[1, 1]

x_dow = np.arange(len(dow_counts))
dow_colors = get_blue_shades(len(dow_counts))

ax_dow.bar(x_dow, dow_counts.values, color=dow_colors)
ax_dow.set_xticks(x_dow)
ax_dow.set_xticklabels(dow_counts.index.tolist(), rotation=45, ha="right", fontsize=7)
ax_dow.set_ylabel("Ticket count", fontsize=8)
ax_dow.set_title("Tickets by day of week", fontsize=8)
ax_dow.tick_params(axis="y", labelsize=7)

for i, v in enumerate(dow_counts.values):
    ax_dow.text(
        i,
        v,
        str(v),
        ha="center",
        va="bottom",
        fontsize=6
    )

ax_ct.set_title("Channel volume by time of day", fontsize=8)
ax_ct.set_xlabel("Time of day bucket", fontsize=8)
ax_ct.set_ylabel("Ticket count", fontsize=8)

ax_ct.set_xticks(x_time)
ax_ct.set_xticklabels(time_buckets, rotation=45, ha="right", fontsize=7)
ax_ct.tick_params(axis="y", labelsize=7)

line_colors = get_blue_shades(len(channels))

for i, ch in enumerate(channels):
    counts = ct_channel_time.loc[ch, time_buckets].values
    ax_ct.plot(
        x_time,
        counts,
        marker="o",
        linestyle="-",
        linewidth=1.5,
        color=line_colors[i],
        label=ch
    )

ax_ct.legend(
    fontsize=6,
    loc="upper center",
    bbox_to_anchor=(0.5, -0.25),
    ncol=2
)

x_seg = np.arange(len(segments))
bottom = np.zeros(len(segments))

product_colors = get_product_colors(len(products))

for i, prod in enumerate(products):
    values = ct_seg_prod_pct[prod].values
    color = product_colors[i]
    ax_seg_prod.bar(
        x_seg,
        values,
        bottom=bottom,
        color=color,
        label=prod
    )
    bottom += values

ax_seg_prod.set_xticks(x_seg)
ax_seg_prod.set_xticklabels(segments, rotation=45, ha="right", fontsize=7)
ax_seg_prod.set_ylabel("Percentage of tickets (%)", fontsize=8)
ax_seg_prod.set_title("Customer segment × product", fontsize=8)
ax_seg_prod.tick_params(axis="y", labelsize=7)

ax_seg_prod.legend(
    fontsize=6,
    loc="upper center",
    bbox_to_anchor=(0.5, -0.20),
    ncol=min(4, len(products))
)

im2 = ax_sent_urg.imshow(
    heat_sent_urg,
    aspect="auto",
    cmap=cmap_blues,
    vmin=0,
    vmax=heat_sent_urg.max(),
    interpolation="nearest"
)

ax_sent_urg.set_xticks(np.arange(len(urg_labels)))
ax_sent_urg.set_xticklabels(urg_labels, fontsize=7)
ax_sent_urg.set_yticks(np.arange(len(sent_labels)))
ax_sent_urg.set_yticklabels(sent_labels, fontsize=7)
ax_sent_urg.set_title("Sentiment × urgency", fontsize=8)
ax_sent_urg.set_xlabel("Urgency", fontsize=8)
ax_sent_urg.set_ylabel("Sentiment", fontsize=8)

for i in range(len(sent_labels)):
    for j in range(len(urg_labels)):
        val = heat_sent_urg[i, j]
        if val > 0:
            ax_sent_urg.text(
                j,
                i,
                str(val),
                ha="center",
                va="center",
                fontsize=6,
                color="black"
            )

cbar2 = fig.colorbar(im2, ax=ax_sent_urg, fraction=0.046, pad=0.04)
cbar2.ax.tick_params(labelsize=6)

plt.tight_layout()
plt.show()

"""# **SECTION 04:** DEFINING *X / Y* VARIABLES AND LABLE ENCODING"""

# Build Input_text (Model Input X)

def build_input_text(row):
    channel = str(row["channel"]).strip()
    segment = str(row["customer_segment"]).strip()
    subject = str(row["subject"]) if pd.notnull(row["subject"]) else ""
    subject = subject.strip()

    dow          = str(row["day_of_week"]).strip()
    time_bucket  = str(row["time_of_day_bucket"]).strip()
    biz_hours    = str(row["is_business_hours"]).strip()
    weekend_flag = str(row["is_weekend"]).strip()

    parts = []
    parts.append(f"[CHANNEL={channel}]")
    parts.append(f"[SEGMENT={segment}]")
    parts.append(f"[DOW={dow}]")
    parts.append(f"[TIME_BUCKET={time_bucket}]")
    parts.append(f"[BUSINESS_HOURS={biz_hours}]")
    parts.append(f"[WEEKEND={weekend_flag}]")

    if subject:
        parts.append(f"[SUBJECT={subject}]")

    parts.append(str(row["ticket_text"]))
    return " ".join(parts)

# Apply to Datasets
df_clean_final["input_text"] = df_clean_final.apply(build_input_text, axis=1)
df_noisy_final["input_text"] = df_noisy_final.apply(build_input_text, axis=1)

print("Example CLEAN input_text:\n")
print(df_clean_final["input_text"].iloc[0][:500])

print("\nExample NOISY input_text:\n")
print(df_noisy_final["input_text"].iloc[0][:500])

# Label Encoding (Y Variables)

label_cols = ["intent", "issue_type", "product", "urgency", "sentiment", "routing_queue"]
encoders = {}

for col in label_cols:
    le = LabelEncoder()
    df_clean_final[col + "_id"] = le.fit_transform(df_clean_final[col])
    encoders[col] = le
    print(f"{col}: {len(le.classes_)} classes")

print("\nSample of encoded targets in CLEAN:")
print(df_clean_final[[c for c in df_clean_final.columns if c.endswith("_id")]].head())

# Apply Same Encoders to Noisy Dataset
for col in label_cols:
    le = encoders[col]

    def safe_transform(x):
        return le.transform([x])[0] if x in le.classes_ else -1

    df_noisy_final[col + "_id"] = df_noisy_final[col].apply(safe_transform)
    df_noisy_final = df_noisy_final[df_noisy_final[col + "_id"] >= 0]

print("\nSizes after encoding:")
print("  CLEAN  :", len(df_clean_final))
print("  NOISY  :", len(df_noisy_final))

print("\nEncoded target columns in CLEAN:")
print([c for c in df_clean_final.columns if c.endswith("_id")])

"""# **SECTION 05:** DEFINE *TRAIN / VALIDATION / TEST* DATASETS"""

# Split Pre-processed Dataset (Train / Validation / Test)

print("Total clean samples:", len(df_clean_final))

# First Split: Train (70%) + Temp (30%)
train_df, temp_df = train_test_split(
    df_clean_final,
    test_size=0.30,
    random_state=42,
    stratify=df_clean_final["issue_type_id"]
)

# Second Split: Val (15%) + Test (15%) from Temp (30%)
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.50,
    random_state=42,
    stratify=temp_df["issue_type_id"]
)

print("Split sizes:")
print("  train:", len(train_df))
print("  val  :", len(val_df))
print("  test :", len(test_df))
print("\nExample distribution check (issue_type_id):")
print("train:")
print(train_df["issue_type_id"].value_counts(normalize=True).head())
print("\nval:")
print(val_df["issue_type_id"].value_counts(normalize=True).head())
print("\ntest:")
print(test_df["issue_type_id"].value_counts(normalize=True).head())

"""# **SECTION 06:** DEFINE MULTI-TASK MODEL"""

# Define Multi-Task Model (DistilBERT with 6 heads)

# Model and Tokenizer Config
MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
MAX_LEN = 256
BATCH_SIZE = 16

# Multi-Task Model Definition
class MultiTaskModel(nn.Module):
    def __init__(self, model_name, num_labels_dict, dropout=0.2):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = self.backbone.config.hidden_size
        self.dropout = nn.Dropout(dropout)

        self.intent_head        = nn.Linear(hidden_size, num_labels_dict["intent"])
        self.issue_type_head    = nn.Linear(hidden_size, num_labels_dict["issue_type"])
        self.product_head       = nn.Linear(hidden_size, num_labels_dict["product"])
        self.urgency_head       = nn.Linear(hidden_size, num_labels_dict["urgency"])
        self.sentiment_head     = nn.Linear(hidden_size, num_labels_dict["sentiment"])
        self.routing_queue_head = nn.Linear(hidden_size, num_labels_dict["routing_queue"])

    def forward(self, input_ids, attention_mask):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0, :]
        x = self.dropout(pooled)

        out_intent        = self.intent_head(x)
        out_issue_type    = self.issue_type_head(x)
        out_product       = self.product_head(x)
        out_urgency       = self.urgency_head(x)
        out_sentiment     = self.sentiment_head(x)
        out_routing_queue = self.routing_queue_head(x)

        return {
            "intent":        out_intent,
            "issue_type":    out_issue_type,
            "product":       out_product,
            "urgency":       out_urgency,
            "sentiment":     out_sentiment,
            "routing_queue": out_routing_queue,
        }

num_labels_dict = {
      "intent":        train_df["intent_id"].nunique(),
      "issue_type":    train_df["issue_type_id"].nunique(),
      "product":       train_df["product_id"].nunique(),
      "urgency":       train_df["urgency_id"].nunique(),
      "sentiment":     train_df["sentiment_id"].nunique(),
      "routing_queue": train_df["routing_queue_id"].nunique(),
 }

# Instantiate Model and Move to GPU/CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultiTaskModel(MODEL_NAME, num_labels_dict).to(device)

print(device)
print(model)

"""# **SECTION 07:** SETTING-UP DATA LOADERS"""

# Create Dataset & DataLoaders

# Custom Dataset Class
class BankingDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=256):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = row["input_text"]

        encoded = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt",
        )

        item = {
            "input_ids":      encoded["input_ids"].squeeze(0),
            "attention_mask": encoded["attention_mask"].squeeze(0),

# Multi-task Labels
            "intent":        torch.tensor(row["intent_id"],        dtype=torch.long),
            "issue_type":    torch.tensor(row["issue_type_id"],    dtype=torch.long),
            "product":       torch.tensor(row["product_id"],       dtype=torch.long),
            "urgency":       torch.tensor(row["urgency_id"],       dtype=torch.long),
            "sentiment":     torch.tensor(row["sentiment_id"],     dtype=torch.long),
            "routing_queue": torch.tensor(row["routing_queue_id"], dtype=torch.long),
        }
        return item

# Create Dataset Objects (Train / Validation / Test)
train_dataset = BankingDataset(train_df, tokenizer, MAX_LEN)
val_dataset   = BankingDataset(val_df,   tokenizer, MAX_LEN)
test_dataset  = BankingDataset(test_df,  tokenizer, MAX_LEN)
noisy_dataset = BankingDataset(df_noisy_final, tokenizer, MAX_LEN)

print("Dataset sizes:")
print("  train:", len(train_dataset))
print("  val  :", len(val_dataset))
print("  test :", len(test_dataset))
print("  noisy :", len(df_noisy_final))

# Wrap in DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)
test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)
noisy_loader = DataLoader(noisy_dataset, batch_size=BATCH_SIZE, shuffle=False)

batch = next(iter(train_loader))
print("\nBatch shapes:")
print("  input_ids      :", batch["input_ids"].shape)
print("  attention_mask :", batch["attention_mask"].shape)
print("  intent         :", batch["intent"].shape)
print("  issue_type     :", batch["issue_type"].shape)

"""# **SECTION 08:** SETTING-UP OPTIMIZER, SCHEDULER AND METRICS"""

# Setup: Optimizer, Scheduler, Metrics, Funcs

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
EPOCHS = 3
LEARNING_RATE = 2e-5

criterion = nn.CrossEntropyLoss()
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps,
)

TASKS = ["intent", "issue_type", "product", "urgency", "sentiment", "routing_queue"]

# Training for ONE Epoch
def train_one_epoch(model, loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0.0

    progress_bar = tqdm(loader, desc="Training", leave=False)

    for batch in progress_bar:
        input_ids      = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        labels = {task: batch[task].to(device) for task in TASKS}

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

# Multi-Task Loss: Average over Tasks (Training)
        loss = 0
        for task_name, y_true in labels.items():
            logits = outputs[task_name]
            loss += criterion(logits, y_true)
        loss = loss / len(TASKS)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        avg_loss_so_far = total_loss / (progress_bar.n + 1)

        progress_bar.set_postfix({
            "batch_loss": f"{loss.item():.4f}",
            "avg_loss":   f"{avg_loss_so_far:.4f}"
        })

    avg_loss = total_loss / len(loader)
    return avg_loss

# Evaluation (Validation / Test / Noisy)
@torch.no_grad()
def eval_one_epoch(model, loader, device, desc="Validation"):
    model.eval()
    total_loss = 0.0

    all_true = {task: [] for task in TASKS}
    all_pred = {task: [] for task in TASKS}

    progress_bar = tqdm(loader, desc=desc, leave=False)

    for batch in progress_bar:
        input_ids      = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        labels = {task: batch[task].to(device) for task in TASKS}

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

# Multi-Task Loss: Average over Tasks (Validation / Test / Noisy)
        loss = 0
        for task_name, y_true in labels.items():
            logits = outputs[task_name]
            loss += criterion(logits, y_true)

            preds = logits.argmax(dim=1)

            all_true[task_name].append(y_true.cpu().numpy())
            all_pred[task_name].append(preds.cpu().numpy())

        loss = loss / len(TASKS)
        total_loss += loss.item()
        avg_loss_so_far = total_loss / (progress_bar.n + 1)

        progress_bar.set_postfix({
            "batch_loss": f"{loss.item():.4f}",
            "avg_loss":   f"{avg_loss_so_far:.4f}"
        })

    avg_loss = total_loss / len(loader)

# Compute Metrics per Task
    metrics = {}
    for task in TASKS:
        y_true = np.concatenate(all_true[task])
        y_pred = np.concatenate(all_pred[task])

        acc = accuracy_score(y_true, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average="macro", zero_division=0
        )

        metrics[task] = {
            "accuracy":        acc,
            "precision_macro": prec,
            "recall_macro":    rec,
            "f1_macro":        f1,
        }

    return avg_loss, metrics

# Multi-task Hamming loss over all 6 outputs
def compute_multi_task_hamming_loss(model, loader, device, tasks):
    model.eval()
    all_correct = {t: [] for t in tasks}

    with torch.no_grad():
        for batch in loader:
            input_ids      = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)

            for t in tasks:
                logits = outputs[t]
                preds  = logits.argmax(dim=1)
                y_true = batch[t].to(device)

                corr = (preds == y_true).cpu().numpy().astype(np.int32)
                all_correct[t].append(corr)

    corr_mat = np.stack(
        [np.concatenate(all_correct[t]) for t in tasks],
        axis=1
    )

    hamming_loss = 1.0 - corr_mat.mean()

    return hamming_loss

"""# **SECTION 09:** MODEL TRAINING AND VALIDATION"""

# Training Loop With Validation

history = {
    "epoch": [],
    "train_loss": [],
    "val_loss": [],
    "val_macro_f1_all_tasks": [],
    "val_macro_prec_all_tasks": [],
}

for epoch in range(1, EPOCHS + 1):
    print(f"\n===== Epoch {epoch}/{EPOCHS} =====")

# Training
    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, device)
    print(f"Train loss: {train_loss:.4f}")

# Validation
    val_loss, val_metrics = eval_one_epoch(model, val_loader, device, desc="Validation")
    print(f"Val loss  : {val_loss:.4f}")

    f1_values   = []
    prec_values = []
    for t in TASKS:
        f1_values.append(val_metrics[t]["f1_macro"])
        prec_values.append(val_metrics[t]["precision_macro"])

    global_f1   = float(np.mean(f1_values))
    global_prec = float(np.mean(prec_values))

    print("  Global macro-F1   (all tasks): {:.3f}".format(global_f1))
    print("  Global macro-Prec (all tasks): {:.3f}".format(global_prec))

    history["epoch"].append(epoch)
    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["val_macro_f1_all_tasks"].append(global_f1)
    history["val_macro_prec_all_tasks"].append(global_prec)

    for task_name, m in val_metrics.items():
        print(f"  [{task_name}] "
              f"acc={m['accuracy']:.3f}, "
              f"prec={m['precision_macro']:.3f}, "
              f"rec={m['recall_macro']:.3f}, "
              f"f1={m['f1_macro']:.3f}")

"""# **SECTION 10:** MODEL TESTING / ROBUSTNESS EVALUATION"""

# Evaluation on Test Dataset

test_loss, test_metrics = eval_one_epoch(
    model,
    test_loader,
    device,
    desc="Test (clean)")

hamming_clean = compute_multi_task_hamming_loss(model, test_loader, device, TASKS)

print(f"\nMulti-task Hamming loss (clean test set): {hamming_clean:.4f}")
print(f"\nTest loss: {test_loss:.4f}")

for task_name, m in test_metrics.items():
    print(f"  [{task_name}] "
          f"acc={m['accuracy']:.3f}, "
          f"prec={m['precision_macro']:.3f}, "
          f"rec={m['recall_macro']:.3f}, "
          f"f1={m['f1_macro']:.3f}")

# Robust Evaluation on Noisy Dataset

noisy_loss, noisy_metrics = eval_one_epoch(
    model,
    noisy_loader,
    device,
    desc="Noisy"
)

hamming_noisy = compute_multi_task_hamming_loss(model, noisy_loader, device, TASKS)
print(f"\nMulti-task Hamming loss (noisy robustness set): {hamming_noisy:.4f}")
print(f"Noisy loss: {noisy_loss:.4f}\n")

for task, m in noisy_metrics.items():
    print(f"  [{task}] "
          f"acc={m['accuracy']:.3f}, "
          f"prec={m['precision_macro']:.3f}, "
          f"rec={m['recall_macro']:.3f}, "
          f"f1={m['f1_macro']:.3f}")

"""# **SECTION 11:** MODEL PERFORMANCE ANALYSIS"""

# Data – F1 Scores per Task
tasks = ["intent", "issue_type", "product", "urgency", "sentiment", "routing_queue"]

f1_clean = {t: test_metrics[t]["f1_macro"]  for t in tasks}
f1_noisy = {t: noisy_metrics[t]["f1_macro"] for t in tasks}

print("Clean F1 macro per task:", f1_clean)
print("Noisy F1 macro per task:", f1_noisy)

clean_vals = np.array([f1_clean[t] for t in tasks])
noisy_vals = np.array([f1_noisy[t] for t in tasks])

clean_color = "#08306b"
noisy_color = "#6baed6"
line_color  = "#9ecae1"

plt.close("all")
fig = plt.figure(figsize=(10, 4.5), dpi=250)

ax_radar = plt.subplot(1, 2, 1, polar=True)
ax_dumb  = plt.subplot(1, 2, 2)

# Radar / spider plot
N = len(tasks)
angles = np.linspace(0, 2 * np.pi, N, endpoint=False)
angles = np.concatenate([angles, [angles[0]]])

clean_r = np.concatenate([clean_vals, [clean_vals[0]]])
noisy_r = np.concatenate([noisy_vals, [noisy_vals[0]]])

ax_radar.plot(angles, clean_r, color=clean_color, linewidth=1.5, label="Clean test F1")
ax_radar.fill(angles, clean_r, color=clean_color, alpha=0.2)

ax_radar.plot(angles, noisy_r, color=noisy_color, linewidth=1.5, label="Noisy F1")
ax_radar.fill(angles, noisy_r, color=noisy_color, alpha=0.2)

ax_radar.set_xticks(angles[:-1])
ax_radar.set_xticklabels(
    [t.replace("_", "\n") for t in tasks],
    fontsize=7
)

ax_radar.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax_radar.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=6)
ax_radar.set_ylim(0, 1.05)

ax_radar.set_title("Multi-task macro-F1 (Clean vs Noisy)", fontsize=8, pad=12)
ax_radar.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1), fontsize=6)

# Dumbbell Chart (Clean vs Noisy F1)
tasks_rev = tasks[::-1]
clean_rev = clean_vals[::-1]
noisy_rev = noisy_vals[::-1]

y_pos = np.arange(len(tasks_rev))

for i in range(len(tasks_rev)):
    ax_dumb.plot(
        [noisy_rev[i], clean_rev[i]],
        [y_pos[i], y_pos[i]],
        color=line_color,
        linewidth=1,
        zorder=1,
    )

ax_dumb.scatter(
    noisy_rev,
    y_pos,
    color=noisy_color,
    s=18,
    label="Noisy F1",
    zorder=2,
)

ax_dumb.scatter(
    clean_rev,
    y_pos,
    color=clean_color,
    s=18,
    label="Clean test F1",
    zorder=3,
)

ax_dumb.set_yticks(y_pos)
ax_dumb.set_yticklabels(
    [t.replace("_", " ") for t in tasks_rev],
    fontsize=7
)
ax_dumb.set_xlabel("Macro-F1", fontsize=8)
ax_dumb.set_xlim(0.5, 1.02)
ax_dumb.set_title("Clean vs Noisy macro-F1 (Dumbbell)", fontsize=8)

ax_dumb.tick_params(axis="x", labelsize=7)
ax_dumb.grid(axis="x", linestyle="--", linewidth=0.4, alpha=0.5)

ax_dumb.legend(loc="lower right", fontsize=6)

plt.tight_layout()
plt.show()

# If not already computed, make sure you have:
# hamming_clean = compute_multi_task_hamming_loss(model, test_loader,  device, TASKS)
# hamming_noisy = compute_multi_task_hamming_loss(model, noisy_loader, device, TASKS)

# 1 - Hamming = overall multi-task accuracy across all 6 labels
multi_acc_clean = 1.0 - hamming_clean
multi_acc_noisy = 1.0 - hamming_noisy

labels = ["Clean test set", "Noisy robustness set"]
x = np.arange(len(labels))

hamming_vals = np.array([hamming_clean, hamming_noisy])
acc_vals     = np.array([multi_acc_clean, multi_acc_noisy])

fig, ax1 = plt.subplots(figsize=(5.0, 3.2), dpi=300)

width = 0.45

bars = ax1.bar(
    x,
    hamming_vals,
    width=width,
    color=["#1f4e79", "#4f81bd"],    # dark → lighter blue
    edgecolor="white",
    linewidth=0.6
)

# Left y-axis: Hamming loss (error)
ax1.set_ylabel("Multi-task Hamming loss\n(overall error rate)", fontsize=7)
ax1.set_xlabel("Evaluation split", fontsize=7)
ax1.set_xticks(x)
ax1.set_xticklabels(labels, fontsize=6)
ax1.tick_params(axis="y", labelsize=6)
ax1.grid(axis="y", linestyle=":", linewidth=0.4, alpha=0.7)

ax1.set_ylim(0, max(hamming_vals) * 1.25 if hamming_vals.max() > 0 else 0.05)

# Annotate bars with error %
for i, b in enumerate(bars):
    height = b.get_height()
    ax1.text(
        b.get_x() + b.get_width() / 2,
        height + 0.01,
        f"{height*100:.1f}%",
        ha="center",
        va="bottom",
        fontsize=6,
        color="black"
    )

# Right y-axis: 1 - Hamming = overall multi-task accuracy
ax2 = ax1.twinx()
ax2.plot(
    x,
    acc_vals,
    marker="o",
    linestyle="-",
    linewidth=1.0,
    markersize=4,
    color="#2f75b5",
    label="1 − Hamming (overall multi-task accuracy)"
)

ax2.tick_params(axis="y", labelsize=6)
ax2.set_ylabel("Overall multi-task accuracy\n(1 − Hamming)", fontsize=7)
ax2.set_ylim(0, 1.05)

# Annotate accuracy points
for xi, yi in zip(x, acc_vals):
    ax2.text(
        xi,
        yi + 0.02,
        f"{yi*100:.1f}%",
        ha="center",
        va="bottom",
        fontsize=6,
        color="#2f75b5"
    )

# Title + legend
ax1.set_title(
    "Multi-task Hamming loss vs. overall accuracy\n(clean vs noisy evaluation)",
    fontsize=8
)

ax2.legend(
    fontsize=6,
    loc="lower center",
    bbox_to_anchor=(0.5, -0.25),
    frameon=True,
    ncol=1
)

plt.tight_layout()
plt.show()

TASKS = ["intent", "issue_type", "product", "urgency", "sentiment", "routing_queue"]

def plot_task_accuracy_matrix_test(loader):
    model.eval()
    all_correct = []

    with torch.no_grad():
        for batch in loader:
            input_ids      = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)

            batch_correct = []
            for t in TASKS:
                y_true = batch[t].cpu().numpy()
                logits = outputs[t]
                y_pred = logits.argmax(dim=1).cpu().numpy()
                correct = (y_pred == y_true)
                batch_correct.append(correct)

            batch_correct = np.stack(batch_correct, axis=1)
            all_correct.append(batch_correct)

    all_correct = np.concatenate(all_correct, axis=0)
    N, n_tasks = all_correct.shape

    M = np.zeros((n_tasks, n_tasks), dtype=float)
    for i in range(n_tasks):
        for j in range(n_tasks):
            both_correct = np.logical_and(all_correct[:, i], all_correct[:, j]).sum()
            M[i, j] = both_correct / N

    norm = Normalize(vmin=0.0, vmax=1.0)

    fig, ax = plt.subplots(figsize=(4.5, 3.5), dpi=300)

    im = ax.imshow(
        M,
        interpolation="nearest",
        cmap="Blues",
        aspect="auto",
        norm=norm
    )

    ax.set_title("Task-level accuracy matrix", fontsize=8)
    ax.set_xlabel("Task j", fontsize=7)
    ax.set_ylabel("Task i", fontsize=7)

    ax.set_xticks(np.arange(n_tasks))
    ax.set_yticks(np.arange(n_tasks))
    ax.set_xticklabels(TASKS, rotation=45, ha="right", fontsize=6)
    ax.set_yticklabels(TASKS, fontsize=6)

    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.ax.tick_params(labelsize=5)
    cbar.set_label("P(task i correct AND task j correct)", fontsize=6)

    max_val = M.max()
    for i in range(n_tasks):
        for j in range(n_tasks):
            val = M[i, j]
            text_color = "white" if val > max_val * 0.5 else "black"
            ax.text(
                j,
                i,
                f"{val*100:.1f}%",
                ha="center",
                va="center",
                fontsize=5,
                color=text_color,
            )

    plt.tight_layout()
    plt.show()

plot_task_accuracy_matrix_test(test_loader)

def compute_per_class_f1(model, loader, device, task_name, encoder):
    model.eval()
    all_y_true = []
    all_y_pred = []

    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            y_true_batch = batch[task_name].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs[task_name]
            y_pred_batch = logits.argmax(dim=1)

            all_y_true.append(y_true_batch.cpu().numpy())
            all_y_pred.append(y_pred_batch.cpu().numpy())

    y_true = np.concatenate(all_y_true)
    y_pred = np.concatenate(all_y_pred)

    f1_per_class = f1_score(y_true, y_pred, average=None, labels=np.arange(len(encoder.classes_)), zero_division=0)
    return f1_per_class, y_true, y_pred

TASK_NAME = "routing_queue"
rq_encoder = encoders[TASK_NAME]
rq_class_labels = rq_encoder.classes_

f1_test_per_class, y_true_test_rq, y_pred_test_rq = compute_per_class_f1(model, test_loader, device, TASK_NAME, rq_encoder)
f1_noisy_per_class, y_true_noisy_rq, y_pred_noisy_rq = compute_per_class_f1(model, noisy_loader, device, TASK_NAME, rq_encoder)

rq_f1_df = pd.DataFrame({
    "class": rq_class_labels,
    "f1_test": f1_test_per_class,
    "f1_noisy": f1_noisy_per_class,
})

rq_f1_df_sorted = rq_f1_df.sort_values(by="f1_test", ascending=False)

class_sorted = rq_f1_df_sorted["class"].tolist()
f1_test_sorted = rq_f1_df_sorted["f1_test"].values
f1_noisy_sorted = rq_f1_df_sorted["f1_noisy"].values

f1_test_macro = f1_score(y_true_test_rq, y_pred_test_rq, average="macro", zero_division=0)
f1_noisy_macro = f1_score(y_true_noisy_rq, y_pred_noisy_rq, average="macro", zero_division=0)

fig, ax = plt.subplots(figsize=(7, 4), dpi=300)

max_chars = 14
wrapped_labels = [
    "\n".join(textwrap.wrap(lbl, max_chars))
    for lbl in class_sorted
]

x = np.arange(len(class_sorted)) # Define x for the bar plot

bar_colors = plt.cm.Blues(np.linspace(0.5, 0.9, len(class_sorted)))
bars = ax.bar(
    x,
    f1_test_sorted,
    color=bar_colors,
    edgecolor="white",
    linewidth=0.5,
    label="Test F1 (clean)"
)

ax.plot(
    x,
    f1_noisy_sorted,
    marker="o",
    linestyle="-",
    linewidth=1.0,
    markersize=3,
    color="#1f4e79",
    label="Noisy F1"
)

ax.axhline(
    y=f1_test_macro,
    color="#0b3c5d",
    linestyle="--",
    linewidth=0.8,
    label=f"Test macro-F1 = {f1_test_macro:.2f}"
)
ax.axhline(
    y=f1_noisy_macro,
    color="#4f88c6",
    linestyle="--",
    linewidth=0.8,
    label=f"Noisy macro-F1 = {f1_noisy_macro:.2f}"
)

ax.set_xticks(x)
ax.set_xticklabels(
    wrapped_labels,
    rotation=65,
    ha="right",
    fontsize=5
)

ax.set_ylabel("F1 score", fontsize=7)
ax.set_xlabel("Routing queues", fontsize=7)
ax.set_ylim(0, 1.05)

ax.set_title("Routing_queue: per-class F1 on test vs noisy sets", fontsize=8)

ax.grid(axis="y", linestyle=":", linewidth=0.4, alpha=0.7)
ax.tick_params(axis="y", labelsize=6)

ax.legend(
    fontsize=6,
    loc="center left",
    bbox_to_anchor=(1.02, 0.5),
    frameon=True
)

plt.tight_layout()
plt.show()

TASK_NAME = "routing_queue"
CONF_THRESHOLD = 0.40
loader = test_loader

def collect_confidence_and_correctness(loader, task_name):
    model.eval()
    all_conf = []
    all_corr = []

    with torch.no_grad():
        for batch in loader:
            input_ids      = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs[task_name]

            probs = torch.softmax(logits, dim=-1)
            conf, pred_idx = probs.max(dim=1)

            conf = conf.cpu().numpy()
            pred_idx = pred_idx.cpu().numpy()
            y_true = batch[task_name].cpu().numpy()
            correct = (pred_idx == y_true).astype(np.int32)

            all_conf.append(conf)
            all_corr.append(correct)

    conf_all = np.concatenate(all_conf)
    corr_all = np.concatenate(all_corr)
    return conf_all, corr_all

conf_all, corr_all = collect_confidence_and_correctness(loader, TASK_NAME)

num_bins = 10
bins = np.linspace(0.0, 1.0, num_bins + 1)

bin_centers = []
bin_acc = []
bin_mean_conf = []
bin_counts = []

for i in range(num_bins):
    mask = (conf_all >= bins[i]) & (conf_all < bins[i+1] if i < num_bins-1 else conf_all <= bins[i+1])
    if mask.sum() == 0:
        continue
    c_bin = conf_all[mask]
    y_bin = corr_all[mask]

    bin_centers.append(0.5 * (bins[i] + bins[i+1]))
    bin_mean_conf.append(c_bin.mean())
    bin_acc.append(y_bin.mean())
    bin_counts.append(mask.sum())

bin_centers = np.array(bin_centers)
bin_mean_conf = np.array(bin_mean_conf)
bin_acc = np.array(bin_acc)
bin_counts = np.array(bin_counts)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8.5, 3.5), dpi=300)

bins_hist = np.linspace(0.0, 1.0, 21)

ax1.hist(
    conf_all,
    bins=bins_hist,
    edgecolor="white",
    linewidth=0.5,
    color="#4f81bd",
)

ax1.axvspan(0.0, CONF_THRESHOLD, color="#b0c4de", alpha=0.6, label="Review Required (<0.40)")
ax1.axvspan(CONF_THRESHOLD, 1.0, color="#1f497d", alpha=0.15, label="OK (≥0.40)")

ax1.axvline(CONF_THRESHOLD, color="#0b3c5d", linestyle="--", linewidth=0.8)

ax1.set_xlabel("Predicted confidence (max softmax)", fontsize=7)
ax1.set_ylabel("Number of predictions", fontsize=7)
ax1.set_title(f"{TASK_NAME} – confidence distribution (test set)", fontsize=8)

ax1.tick_params(axis="both", labelsize=6)
ax1.legend(fontsize=6, loc="upper center", frameon=True)

ax2.plot([0, 1], [0, 1], linestyle="--", color="#888888", linewidth=0.8, label="Perfect calibration")

ax2.plot(
    bin_mean_conf,
    bin_acc,
    marker="o",
    linestyle="-",
    linewidth=1.0,
    markersize=3,
    color="#1f4e79",
    label=f"{TASK_NAME} (binned)"
)

for x_c, y_c, n_c in zip(bin_mean_conf, bin_acc, bin_counts):
    ax2.text(
        x_c,
        y_c + 0.03,
        f"n={n_c}",
        ha="center",
        va="bottom",
        fontsize=5,
        color="black"
    )

ax2.set_xlim(0, 1)
ax2.set_ylim(0, 1.05)
ax2.set_xlabel("Mean predicted confidence (per bin)", fontsize=7)
ax2.set_ylabel("Empirical accuracy (per bin)", fontsize=7)
ax2.set_title(f"{TASK_NAME} – reliability curve (test set)", fontsize=8)

ax2.tick_params(axis="both", labelsize=6)
ax2.legend(fontsize=6, loc="lower right", frameon=True)

plt.tight_layout()
plt.show()

epochs = np.array(history["epoch"])
train_losses = np.array(history["train_loss"])
val_losses   = np.array(history["val_loss"])

val_f1_macro_all   = np.array(history["val_macro_f1_all_tasks"])
val_prec_macro_all = np.array(history["val_macro_prec_all_tasks"])

print("Global macro-F1  :", np.round(val_f1_macro_all,   3))
print("Global macro-Prec:", np.round(val_prec_macro_all, 3))

x_smooth = np.linspace(epochs.min(), epochs.max(), 200)

poly_train = Polynomial.fit(epochs, train_losses, deg=2)
poly_val   = Polynomial.fit(epochs, val_losses,   deg=2)
train_smooth = poly_train(x_smooth)
val_smooth   = poly_val(x_smooth)

poly_f1   = Polynomial.fit(epochs, val_f1_macro_all,   deg=2)
poly_prec = Polynomial.fit(epochs, val_prec_macro_all, deg=2)
f1_smooth   = poly_f1(x_smooth)
prec_smooth = poly_prec(x_smooth)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8.5, 3.5), dpi=300)

ax1.plot(
    x_smooth,
    train_smooth,
    linestyle="-",
    linewidth=1.2,
    color="#1f4e79",
    label="Train loss"
)
ax1.plot(
    x_smooth,
    val_smooth,
    linestyle="-",
    linewidth=1.2,
    color="#4f81bd",
    label="Validation loss"
)

ax1.scatter(epochs, train_losses, color="#1f4e79", s=12)
ax1.scatter(epochs, val_losses,   color="#4f81bd", s=12)

ax1.set_xlabel("Epoch", fontsize=7)
ax1.set_ylabel("Average cross-entropy loss (all tasks)", fontsize=7)
ax1.set_title("Learning curve – loss (multi-task model)", fontsize=8)
ax1.tick_params(axis="both", labelsize=6)
ax1.grid(axis="y", linestyle=":", linewidth=0.4, alpha=0.7)
ax1.legend(fontsize=6, loc="upper right", frameon=True)

ax2.plot(
    x_smooth,
    f1_smooth,
    linestyle="-",
    linewidth=1.2,
    color="#2f75b5",
    label="Validation macro-F1 (all tasks)"
)
ax2.plot(
    x_smooth,
    prec_smooth,
    linestyle="-",
    linewidth=1.2,
    color="#6d9eeb",
    label="Validation macro-Precision (all tasks)"
)

ax2.scatter(epochs, val_f1_macro_all,   color="#2f75b5", s=12)
ax2.scatter(epochs, val_prec_macro_all, color="#6d9eeb", s=12)

ax2.set_xlabel("Epoch", fontsize=7)
ax2.set_ylabel("Macro score (validation)", fontsize=7)
ax2.set_title("Learning curve – global macro-F1 & macro-Precision", fontsize=8)
ax2.set_ylim(0.90, 1.00)
ax2.set_yticks(np.linspace(0.90, 1.00, 6))

ax2.tick_params(axis="both", labelsize=6)
ax2.grid(axis="y", linestyle=":", linewidth=0.4, alpha=0.7)

ax2.legend(
    fontsize=6,
    loc="lower right",
    frameon=True
)

plt.tight_layout()
plt.show()

"""# **SECTION 12:** SAVE MODEL AND ENCODERS"""

# SAVE Trained Model

MODEL_PATH = "/content/drive/MyDrive/Colab Notebooks/TrainedModel/multitask_distilbert_clean.pt"

checkpoint = {
    "model_state_dict": model.state_dict(),
    "model_name": "distilbert-base-uncased",
    "num_labels_dict": {
        "intent":        train_df["intent_id"].nunique(),
        "issue_type":    train_df["issue_type_id"].nunique(),
        "product":       train_df["product_id"].nunique(),
        "urgency":       train_df["urgency_id"].nunique(),
        "sentiment":     train_df["sentiment_id"].nunique(),
        "routing_queue": train_df["routing_queue_id"].nunique(),
    },
}

# SAVE Encoders
SAVE_PATH = "/content/drive/MyDrive/Colab Notebooks/TrainedModel/label_encoders.pkl"

with open(SAVE_PATH, "wb") as f:
    pickle.dump(encoders, f)

torch.save(checkpoint, MODEL_PATH)
print(f"Model saved to {MODEL_PATH}")
print("Encoders saved to:", SAVE_PATH)